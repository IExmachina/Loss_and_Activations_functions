{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activations_and_Loss_functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-sQMTPqWym"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "Algumas das funções de ativação para redes neurais. Elas recebem o resultado da função soma da camada anterior e retornam um valor transformado de acordo com a função para a próxima camada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wunKJeSjg1Tk"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgi3T-qeiBRu"
      },
      "source": [
        "## Step Function\n",
        "**Recomendação:** Classificação binária.\n",
        "\n",
        "**Retorno :** Retorna 0 ou 1 somente.\n",
        " \n",
        "**Descrição:** Não ultilizada em Deep Learning pois só trabalha com problemas linearmente separáveis.\n",
        "Retorna o valor 1 quando a soma é igual ou maior que 1, e retorna 0 quando a soma é menor que 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p1sW03Cg7WD",
        "outputId": "91c3e712-d076-48ff-c5a6-4596cf3b772c"
      },
      "source": [
        "def StepFunction(soma):\n",
        "  if soma >= 1:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "\n",
        "StepFunction(7), StepFunction(-10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOdG1jyTiEBA"
      },
      "source": [
        "#### Sigmoid\n",
        " **Recomendação:** Classificação binária.\n",
        "\n",
        "**Retorno :** Retorna valores entre 0 e 1.\n",
        " \n",
        "**Descrição:** Muito ultilizada para classificação binária, inclusive é possivel ver a probabilidade para a classe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAftc3gbiHSp",
        "outputId": "71713d7b-b394-4187-e4b5-da1dae990efc"
      },
      "source": [
        "def SigmoidFunction(soma):\n",
        "  return 1 / ( 1 + np.exp(-soma))\n",
        "\n",
        "\n",
        "SigmoidFunction(0.267)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5663562616210012"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iOqdp5GjKH6"
      },
      "source": [
        "#### Hyperbolic Tangent\n",
        " **Recomendação:** Classificação binária.\n",
        "\n",
        "**Retorno :** Retorna valores entre -1 e 1.\n",
        " \n",
        "**Descrição:** Comunmente usada para classificação binária, boa para quando temos muitos valores negativos nos dados. Seu diferencial é que valores negativos serão mapeados negativos mesmo, mas dentro da escala de até -1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z0HdHzBivVr",
        "outputId": "ee9365cf-9862-4d08-bc35-4d6a8e7b2970"
      },
      "source": [
        "def tahnFunction(soma):\n",
        "  return (np.exp(soma) - np.exp(-soma)) / (np.exp(soma) + np.exp(-soma))\n",
        "\n",
        "\n",
        "tahnFunction(12), tahnFunction(-12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9999999999244972, -0.9999999999244972)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwUz55Y7m0EW"
      },
      "source": [
        "#### Relu (Rectified Linear Units)\n",
        "Recomendação: Redes neurais convolucionais (Visão computacional) e redes neurais profundas (muitas camadas escondidas).\n",
        "\n",
        "**Retorno :** Retorna um valor igual ou maior que zero.\n",
        " \n",
        "**Descrição:** Uma das funções mais usadas, principalmente em redes neurais profundas. Normalmente traz ótimos resultados mas se for ultilizada em dados com valores negativos, não irá ter bons resultados já que ela \"zera\" a entradas negativas as tornando inuteis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogTzy1IdlI_Z",
        "outputId": "ac0d1ea5-7be3-4e52-e77c-09619eff9c83"
      },
      "source": [
        "def reluFunction(soma):\n",
        "  if soma >= 0:\n",
        "    return soma\n",
        "  return 0\n",
        "\n",
        "\n",
        "reluFunction(7), reluFunction(-10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9qH_JY8okK1"
      },
      "source": [
        "#### Linear\n",
        " **Recomendação:** Regressão.\n",
        "\n",
        "**Retorno :** Retorna o mesmo valor, sem alteração.\n",
        " \n",
        "**Descrição:** Muita ultilizada para regressão pois em casos como prever preços por exemplo, retorna um valor equivalente e não em escala."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcD1gP1qn4aB",
        "outputId": "c76a880f-e3fb-46da-cf34-65e9381fb8d8"
      },
      "source": [
        "def LinearFunction(soma):\n",
        "  return soma\n",
        "\n",
        "\n",
        "LinearFunction(7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkgUe7Vopui9"
      },
      "source": [
        "####Softmax\n",
        " **Recomendação:** Classificação com mais de duas classes e redes neurais convolucionais.\n",
        "\n",
        " **Retorno :** Retorna um valor de probabilidade para cada classe. A soma das probabilidades retornadas é igual a 1.\n",
        " \n",
        "**Descrição:** Talvez a função mais usada em Deep Learning. Ela retorna a probabilidade para cada classe por isso recebe mais de um valor de soma, isso na camada de saída. Deve receber um vetor com os valores por exemplo.\n",
        "\n",
        "**Nota:** Ultilizada primariamente na camada de sáida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbGBDLr8polY",
        "outputId": "c0095cc3-565b-4b3e-e0a5-afbf4fb4eb6e"
      },
      "source": [
        "def SoftmaxFunction(vetor):\n",
        "  ex = np.exp(valores)\n",
        "  return ex / ex.sum()\n",
        "\n",
        "\n",
        "valores = [2.1, 3.4 , 7.2, 2.6]\n",
        "SoftmaxFunction(valores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00587061, 0.02154103, 0.96290935, 0.00967901])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss functions\n",
        "Antes de apresentar as loss functions, vou criar vetores e matrizes aleatorios, para simular uma rede neural e seus parametros (W e B).\n",
        "\n",
        "Assim como vou criar X e Y, que irão simular um dataset.\n"
      ],
      "metadata": {
        "id": "p49B2B6jpBV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inicializar_parametros(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- Array Python (lista) contendo as dimensões das cada camada da rede neural.\n",
        "    Ex: Se layer_dims = [5, 4, 4, 2], temos temos uam rede neural como 4 camadas. Camada 1 com 5 neurônios, camada 2 com 4 neurônios, e assim sucessivamente.\n",
        "    \n",
        "    Retorna:\n",
        "    parameters -- Dicionário python, contendo os parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    onde WL e bL representam a matrix W(pesos) e o vetor b(bias) da última camada da rede neural.\n",
        "                    Wl -- Matrix/vetor de pesos com dimensão (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- Vetor de bias com dimensão (layer_dims[l], 1)\n",
        "\n",
        "    X = Matriz aleatória numpy simulando um dataset. Terá dimensões 5x5  \n",
        "    y = Array numpy simulando as classes de um dataset. Terá dimensões de 5x1, sendo 0 ou 1.             \n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3) # Para ter os mesmos resultados sempre.\n",
        "    parametros = {}\n",
        "    L = len(layer_dims) # Número de camadas da rede neural\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parametros['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01    \n",
        "        parametros[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "    X = np.random.randint(0,5, size = (5,5))\n",
        "    y = np.random.randint(2, size = (5,1))\n",
        "\n",
        "    # Simulando o output da forward propagation. (Deveria ser os parametros da ultima camada(WL e bL), mas para simplificar vou usar os paranmetros da primeira camada mesmo)\n",
        "    AL = np.dot(X, parametros['W1']) + parametros['b1']\n",
        "    AL = SigmoidFunction(AL)\n",
        "\n",
        "        \n",
        "    return parametros, AL, y"
      ],
      "metadata": {
        "id": "XvFIu2e5-3J_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_dims = [5, 5, 4, 2]\n",
        "parametros, AL, y = inicializar_parametros(layer_dims)"
      ],
      "metadata": {
        "id": "p6ogSOBdFugg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parametros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29pMCGtgG3h1",
        "outputId": "bdccdb52-4de0-48aa-a90d-039c6087a666"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493, -0.00277388],\n",
              "        [-0.00354759, -0.00082741, -0.00627001, -0.00043818, -0.00477218],\n",
              "        [-0.01313865,  0.00884622,  0.00881318,  0.01709573,  0.00050034],\n",
              "        [-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068],\n",
              "        [-0.01185047, -0.0020565 ,  0.01486148,  0.00236716, -0.01023785]]),\n",
              " 'W2': array([[-0.00712993,  0.00625245, -0.00160513, -0.00768836, -0.00230031],\n",
              "        [ 0.00745056,  0.01976111, -0.01244123, -0.00626417, -0.00803766],\n",
              "        [-0.02419083, -0.00923792, -0.01023876,  0.01123978, -0.00131914],\n",
              "        [-0.01623285,  0.00646675, -0.00356271, -0.01743141, -0.0059665 ]]),\n",
              " 'W3': array([[-0.00588594, -0.00873882,  0.00029714, -0.02248258],\n",
              "        [-0.00267762,  0.01013183,  0.00852798,  0.01108187]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b3': array([[0.],\n",
              "        [0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2JZiryrG_sS",
        "outputId": "b479306b-7877-43c5-8c0d-b1e2bb211c03"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5109275 , 0.50459256, 0.49459978, 0.48776873, 0.49102036],\n",
              "       [0.49602577, 0.49812248, 0.49984918, 0.50304767, 0.49468807],\n",
              "       [0.4980603 , 0.50451779, 0.49954083, 0.50519394, 0.48148024],\n",
              "       [0.47099671, 0.50133612, 0.50820915, 0.52925312, 0.47926371],\n",
              "       [0.5053714 , 0.51037771, 0.50287232, 0.49385854, 0.49402252]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n72TwJHjHBQU",
        "outputId": "da02a36e-f512-455d-a0b9-08ab3aafd351"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-entropy \n",
        "\n",
        "\n",
        "$$J  =  -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
        "\n",
        "* **J ->** Comumente usado para denotar o valor de custo.\n",
        "* **m ->** Numero de exemplos ou tamanho do dataset.\n",
        "* **y -> A** \"groud truth\", a verdadeira classe do respectivo exemplo(x).\n",
        "* **aL ->** \"a\" é o resultado da camada anterior da rede neural, \"l\" vem de layer e representa a respectiva camada da rede neural, no caso \"L\" representa a última camada da rede. Então \"aL\" é o valor de saída da rede neural, também conhecido como yhat.\n",
        "* **log ->** logaritmo natural (a base é o número de Euler).\n",
        "* **i ->** representa cada exemplo, diferente do \"m\" que representa todos.\n",
        "\n",
        "\n",
        "A cross-entropy cost function, é uma função de custo* usada na camada de saída para retornar um valor de  probabilidade para cada classe.\n",
        "Há algumas variações desta função, e também variações de nomes (Ex:Log loss).\n",
        "Quando usada com a função de ativação softmax, é cahamda de \"categorical cross entropy\" que é a mais apropriada para classificações com multiplas classes.\n",
        "Temos pequenas variações na hora de computar o gradiente também, dependendo de qual cross-entropy está sendo usada e qual função de ativação.\n",
        "Para classificação binária, cross entropy e sigmoid é uma combinação eficaz e recomendada. Com esta combinação, cross entropy passa a se chamar Binary cross entropy. \n",
        "\n",
        "Fonte: https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
        "\n",
        "\n",
        "**Nota1:** A cross-entropy é chamada de cost function, mas cost e loss function são a mesma coisa, só \"diferem\" em uma coisa. Normalmente se usa o termo cost quanto se trata da **MÉDIA** do valor da loss function, sobre todo os exemplos(dados de treino). Enquanto loss function/error function é usado quando se trata de somente um exemplo. \n",
        "\n",
        "**Nota 2:** É muito comum usar a L2 loss function junto com a cross-entropy, para regularização (diminuição da variância do modelo). Para isso, somamos o valor de custo da cross-entropy com o valor de custo da L2.  \n",
        "\n",
        "Com L2:\n",
        "$$J_{withL2} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost}$$\n",
        "\n",
        "* **λ ->** Hiperparâmetro lambda, é um valor escalar.\n",
        "\n",
        "* **W ->** São os pesos da camada em questão, elevados ao quadrado.\n",
        "Na formula, temos a soma de W[l] ao quadrado para cada \"w\" da rede neural, como na rede tem 3 camadas ocultas, temos w[l]l, w[l]k e w[l]j.\n",
        "* **l, j e k ->** Representam as camadas da rede neural. Se \"k\" é a camada um, então W[l]k representa W1, que uma matriz ou vetor com os pesos da camada 1."
      ],
      "metadata": {
        "id": "rKcnhSestMMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "17919bb7d82635554b52aed7e96e8d9b",
          "grade": false,
          "grade_id": "cell-abad606772066f14",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ZofvOeyapKuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7034e95c-8fd0-49f6-8e38-4df0d7edcd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O valor de custo usando a função cross-entropy é: 17.316031767206688\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def cross_entropy_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implementa a função de custo \"Cross-Entropy\".\n",
        "\n",
        "    Argumentos:\n",
        "    AL -- Output do processo de forward propagation, é um vetor de probabilidades correspondentes as previsões para cada classe.\n",
        "    Y -- Vetor com a verdadeira classe. (Por exemplo: Contendo 0 não é gato, ou 1 para gato).\n",
        "\n",
        "    Retorna:\n",
        "    J -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # o numero de exemplos contidos no dataset.\n",
        "\n",
        "   \n",
        "    # Computa a cost function sem a regularização(L2).\n",
        "    cost = np.dot((-1 / m), np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL))))\n",
        "\n",
        "    \n",
        "    \n",
        "    cost = np.squeeze(cost)   # Para garantir a dimensão correta. (Transforma [[10]] em 10).\n",
        "\n",
        "    \n",
        "    return cost\n",
        "\n",
        "print(f'O valor de custo usando a função cross-entropy é: {cross_entropy_cost(AL, y)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_cost_with_l2(AL, Y, parametros, lambd):\n",
        "    \"\"\"\n",
        "    Implementa a função de custo \"Cross-Entropy\" com regularização L2.\n",
        "    \n",
        "    Argumentos:\n",
        "     AL -- Output do processo de forward propagation, é um vetor de probabilidades correspondentes as previsões para cada classe.\n",
        "     Y -- Vetor com a verdadeira classe. (Por exemplo: Contendo 0 não é gato, ou 1 para gato).\n",
        "     lambd -- Hiperparâmetro de regularização, um escalar.\n",
        "     parametros -- representa um dicionário com os parametros da rede neural, neste caso, precisamos dos valores de W1, W2 e W3, \n",
        "     que são vetores com os valores dos pesos das respectivas camadas da rede neural. (Neste exemplo a rede neural tem 3 camadas)\n",
        "\n",
        "    Retorna:\n",
        "    J_regularizado -- cross-entropy cost com regularização L2.\n",
        "    cost_with_CE -- cross-entropy sem regularização.\n",
        "    L2_regularization_cost -- valor da função de custo L2\n",
        "\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]    # o numero de exemplos contidos no dataset.\n",
        "    W1 = parametros[\"W1\"]  # Matriz W1, com os pesos da primeira camada oculta da rede.\n",
        "    W2 = parametros[\"W2\"]  # Matriz W2, com os pesos da segunda camada oculta da rede.\n",
        "    W3 = parametros[\"W3\"]  # Matriz W3, com os pesos da terceira camada oculta da rede.\n",
        "\n",
        "\n",
        "    # Retorna o resultado da cross entropy cost, função criada acima.\n",
        "    cost_with_CE = cross_entropy_cost(AL, Y) \n",
        "    \n",
        "    # Cálculo da função de custo L2.\n",
        "    L2_regularization_cost = np.dot(1 / m, lambd / 2) * (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))\n",
        "    \n",
        "    \n",
        "    # Como vimos na equação, cross entropy com regularização L2 é simplesmente a soma entre os respectivos resultados.\n",
        "    J_regularizado = cost_with_CE + L2_regularization_cost\n",
        "    \n",
        "    return J_regularizado, cost_with_CE, L2_regularization_cost\n",
        "\n",
        "J_r, ce, l2 = cross_entropy_cost_with_l2(AL, y, parametros, lambd=0.7)\n",
        "print(f'J sem regularização: {ce}')\n",
        "print(f'Valor da loss L2: {l2}')\n",
        "print(f'Valor de J regularizado (Soma da cross-entropy com L2): {J_r}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuZdC0ab7CUU",
        "outputId": "5299ce66-5a21-4882-f486-c7c3091b2b5f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J sem regularização: 17.316031767206688\n",
            "Valor da loss L2: 0.0020001862588035803\n",
            "Valor de J regularizado (Soma da cross-entropy com L2): 17.318031953465493\n"
          ]
        }
      ]
    }
  ]
}